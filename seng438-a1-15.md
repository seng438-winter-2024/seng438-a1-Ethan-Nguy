>   **SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#1 – Introduction to Testing and Defect Tracking**

| Group: 15      |
|-----------------|
| Ethan Nguy                |   
| Wahid Hoji Aminullah              |   
| James Platt               |   
| Mariya Podgaietska                |   


**Table of Contents**

(When you finish writing, update the following list using right click, then
“Update Field”)

[1 Introduction	1](#Introduction)

[2 High-level description of the exploratory testing plan	1](#High-level-description-of-the-exploratory-testing-plan)

[3 Comparison of exploratory and manual functional testing	1](#)

[4 Notes and discussion of the peer reviews of defect reports	1](#_Toc439194680)

[5 How the pair testing was managed and team work/effort was
divided	1](#_Toc439194681)

[6 Difficulties encountered, challenges overcome, and lessons
learned	1](#_Toc439194682)

[7 Comments/feedback on the lab and lab document itself	1](#_Toc439194683)

# Introduction

This lab serves as an introduction to testing and defect tracking, encompassing four main components: familiarization with the system under test (ATM Simulation) and the defect tracking system (our team used Jira), exploratory testing, manual scripted testing, and regression testing. The primary focus of this lab is on human-based testing. 

Exploratory testing, which is a variant of human-based testing, involves designing and executing tests simultaneously in an open manner, devoid of extensive planning or documentation. This manual testing approach aims to create realistic use case scenarios for the system. While exploratory testing is executed without predefined test scripts, it's crucial to note that adequate preparation through reading the system requirements is essential for its effective implementation. 

Similarly, manual scripted testing is another facet of human-based testing. In this approach, testers adhere to a pre-written script during execution. That is we first create test cases, and then we create the test scripts. Next, we associate the test scripts with the test cases. Thus during execution, the script opens and guides the tester through the test step-by-step. The testing script encompasses meticulously designed and reviewed test cases and steps, with documentation of the test cases. The test cases are often formulated based on the system requirements. 

# High-level description of the exploratory testing plan
Our testing aims to assess the ATM Simulation System with respect to the stipulated requirements outlined in Appendix B of the Assignment 1 handout, titled “High Level Requirements” and use case associated with each Actor within the ATM System. 

Starting with the Operator, who possesses two key use cases, that is System Startup and System Shutdown, our testing approach will encompass both common and exceptional paths for these use cases (such as edge cases). 

Subsequently, we will initiate exploratory testing for the customer. The customer’s use cases can be broken down into two, that is the Session (time they spend on the ATM) and Transaction (what they do while they use the ATM) which will go through thorough examination. Our testing technique will extend to cover the requirements for these use cases, as well as their extended and inherited counterparts. 

# Comparison-of-exploratory-and-manual-functional-testing

Exploratory and Scripted testing both provide benefits and drawbacks for the quality assurance tester. Exploratory testing puts the SUT in a situation closer to a real world scenario than scripted testing, as there is no set order or type of actions the tester has to take. This makes exploratory testing superior at revealing defects that a user would notice and feel the impact of immediately. In our experience, Exploratory testing also revealed more bugs than scripted testing, making it a more effective form of testing. 

Scripted testing proved to be more efficient than Exploratory testing though, as we were able to cover more functionalities and use cases over the same period of time with Scripted testing than Exploratory testing. Scripted testing was also better able to cover edge cases, as the tests were designed to cover cases and bugs that most likely would be missed under regular use (exploratory.) 

The main tradeoff between Exploratory and Scripted testing seems to be time. Scripted testing takes significantly less time than Exploratory testing does to cover the same functionality. This increase in efficiency comes at the cost of coverage, as Exploratory testing tests the applications as if it was in the real world. This means that tests are not isolated to a single functionality; tests are done on top of each other, revealing bugs that are caused by the actions taken early by the user. These kinds of ‘compounding’ bugs are more likely to be caught by Exploratory testing, at the expense of the testers time.


# Notes and discussion of the peer reviews of defect reports

Peer review of each the defect reports was fluid and easy to do. Following a specific structure of how bugs were reported allowed easy knowledge of how bugs may be replicated. This allowed us to find bugs that were duplicated between groups during the exploratory testing phase of the tests. Peer reviews allowed a group member to provide clarity to each bug report to ensure that the bug report made sense logically and clearly.

# How the pair testing was managed and team work/effort was divided 

Pair testing was done in two groups: Group 1 - Mariia and Ethan, and Group 2 - James and Wahid. The two groups spent roughly the same amount of time of exploratory testing for both versions, although group 2 had some issues getting version 1.0 to run. This meant that Group 1 did more work than Group 2 during the first round of exploratory testing.
	
After 30-40 minutes of exploratory testing on version 1.0, the groups paused to compare bugs. After comparing bugs, the team worked as one once again for the Manual scripted testing, before moving on to Exploratory testing on version 1.1 of the SUT. After another round of Exploratory testing, the two groups came back together once again to compare bugs and work on marking 1.0 tickets as active or solved.

During the exploratory testing itself, the two groups did not divide up the application. Both groups tested the entire application thoroughly, trying to verify the applications functionality independently of each other. Both groups tested the application with both accounts, and tried to cover as much functionality as possible. 

To make up for the lag in productivity on the part of Group 2 during the first session of Exploratory testing, Group 2 took on a larger share of the work involved with completing the Defect report and Lab report.


# Difficulties encountered, challenges overcome, and lessons learned

Learning how to operate Atlassia Jira was a large challenge that was difficult to overcome, however as a group we were able to get through it. Navigating a software relatively blind was difficult but as a group we found how to report issues properly. 

Another one of the difficulties that we encountered as a group was deciding how exactly to report bugs that had changed within regression testing. In other words, bugs that were fixed but changed to become new bugs. Communication was the key to overcoming this and we decided on reporting these bugs as new bugs while considering the old bugs as fixed. Although this has less to do with the testing, a lesson that could be learned from this is that when a bug is fixed, it does not necessarily mean the entire issue is resolved.


# Comments/feedback on the lab and lab document itself

The lab was an excellent and practical opportunity to apply the theoretical knowledge gained in class to the real world. It provided a valuable insight into the initial steps of testing real-life systems. The lab document itself was well structured, offering clear and detailed instructions that guided us through each step of the process. This not only made it easier to complete the lab but also enhanced our understanding by breaking down the process of testing into manageable steps. 
